{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is a more detailed description of the algorithm we will implement in `c++` & `CUDA` for reference. It contains: \n",
    "- Two identic approaches to the algorithm:\n",
    "    - [One](#basic-numpy) using classic `numpy` and\n",
    "    - [the other](#cupy-implementation) using `cuPY`, the `numpy` counterpart for `CUDA`\n",
    "- [Benchmarks](#benchmark) for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Numpy\n",
    "We start with the `numpy` implementation, these are the steps we will follow:\n",
    "- Generate the samples\n",
    "- Build the estimator:\n",
    "    - Define the ranges for the parameters: In the real world we would need to play a bit with the numbers by starting with a very wide range as we won't know where $\\mu$ and $\\sigma$ fall. But, since this is a contrived example, we can be a bit self-indulgent and use the information we defined to generate the random variates.\n",
    "    - With these ranges we then build the outer product so we will have all the combinations for $\\mu$, $\\sigma$ & $x$ \n",
    "    - Build the prior: for the sake of simplicity we use a flat prior which won't have impact on the posterior. Had we had a reasonable assumption that the parameters are these or those, we should have set it as our prior.\n",
    "    - Compute the likelihood: this step involves two operations:\n",
    "        - First we compute the densities for each triplet of $\\mu$, $\\sigma$ & $x$, where $x$ are the observations.\n",
    "        - Second, we reduce the densities to a single number by taking the product over the second axis, ie, the densities. This is because we can think of those densities as the joint probability of observation 1 and observation 2...observation $n$. This product can become really small, so if we were only interested in the point estimates, we could take the log-likelihood by using `np.log(densities).sum(axis=2)`. However, since we don't have too many observations and parameter pairs, we will stick to the conventional product.\n",
    "    - Compute the posterior: we do the classic $\\mathrm{prior}\\times\\mathrm{likelihood}/\\mathrm{evidence}$, but notice that the product is not really necessary as we are using a flat prior, so a bare normalization will do the trick as well.\n",
    "- Print the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 20, 2  # the values we want to infer\n",
    "np.random.seed(42)  # Let's fix the samples for reproduction purposes\n",
    "\n",
    "# Generate and display some of the observations\n",
    "observations_np = np.random.normal(mu, sigma, 50).astype(np.float32)\n",
    "observations_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_estimator(observations):\n",
    "    \"\"\"\n",
    "    Compute the posterior distribution of a set of observations.\n",
    "\n",
    "    Arguments:\n",
    "        - observations (np.ndarray): a numpy array containing the the observations\n",
    "\n",
    "    Returns:\n",
    "        - a tuple of np.ndarray:\n",
    "            - posterior (np.ndarray[101]): the posterior probability of the observations.\n",
    "            - mu_range (np.ndarray[101]): the range of parameters used for the mean.\n",
    "            - sigma_range (np.ndarray[101]): the range of parameters used for the std.\n",
    "    \"\"\"\n",
    "    # Define the ranges\n",
    "    ranges_size = 101\n",
    "    mu_range = np.linspace(18, 22, ranges_size, dtype=np.float32)\n",
    "    sigma_range = np.linspace(1, 3, ranges_size, dtype=np.float32)\n",
    "\n",
    "    # Build the outer product\n",
    "    mu_grid, sigma_grid, observations_grid = np.meshgrid(\n",
    "        mu_range, sigma_range, observations\n",
    "    )\n",
    "\n",
    "    # build the prior\n",
    "    prior = np.ones((ranges_size, ranges_size))\n",
    "\n",
    "    # compute the likelihood\n",
    "    densities = norm(mu_grid, sigma_grid).pdf(observations_grid)\n",
    "    likes = densities.prod(axis=2)\n",
    "    # Alternatively we could use the log-likelihood, but it will spoil the marginals.\n",
    "    # np.log(densities).sum(axis=2)\n",
    "\n",
    "    # Compute posteriors.\n",
    "    posterior = likes * prior  # this guy has no effect\n",
    "    posterior /= posterior.sum()  # normalization\n",
    "\n",
    "    # Return the expectations\n",
    "    return posterior, mu_range, sigma_range\n",
    "\n",
    "posterior, mu_range, sigma_range = numpy_estimator(observations_np)\n",
    "print(\n",
    "    np.sum(mu_range * posterior.sum(axis=1)),\n",
    "    np.sum(sigma_range * posterior.sum(axis=0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "One of the nice things of doing Bayesian Statistics is that one does not get just a point estimate but rather a distribution that gives a feeling of how confident one should be about that point estimate.\n",
    "\n",
    "In the following plots, we display this confidence as a:\n",
    "- Posterior joint distribution among `mu_range` and `sigma_range`\n",
    "- Marginals for `mu` and `sigma`\n",
    "\n",
    "Computing these credible intervals is out of the scope of this project but it's something very straightforward to do by means of the CDF:\n",
    "\n",
    "```python\n",
    "def credible_interval_90(distribution):\n",
    "    \"\"\"Return the values that lay in the 90% of the distribution.\"\"\"\n",
    "\n",
    "    def find_quantile(p):\n",
    "        \"\"\"Get the index where the cdf overcomes some prob p.\"\"\"\n",
    "        cdf = distribution.cumsum()\n",
    "        quantile = distribution[cdf > p][0]\n",
    "        return np.where(distribution == quantile)[0][0]\n",
    "\n",
    "    lo, hi = [find_quantile(p) for p in (.05, .95)]\n",
    "    return distribution[lo:hi]\n",
    "```\n",
    "\n",
    "You can check [this source](https://allendowney.github.io/ThinkBayes2/chap05.html#credible-intervals) to know more about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "df = pd.DataFrame(posterior, index=sigma_range.round(2), columns=mu_range.round(2))\n",
    "_, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))\n",
    "ax1 = sns.heatmap(df, cmap=\"YlGnBu\", ax=ax[0])\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_title(\"Posterior Joint Distribution\")\n",
    "ax1.set_xlabel(\"mu\")\n",
    "ax1.set_ylabel(\"sigma\")\n",
    "\n",
    "ax[1].plot(mu_range, posterior.mean(axis=1))\n",
    "ax[1].set_title(\"Mu Marginal\")\n",
    "ax[1].set_xlabel(\"$\\mu$\")\n",
    "ax[1].set_ylabel(\"$p(\\mu|data)$\")\n",
    "\n",
    "ax[2].plot(sigma_range, posterior.mean(axis=0))\n",
    "ax[2].set_title(\"Sigma Marginal\")\n",
    "ax[2].set_xlabel(\"$\\sigma$\")\n",
    "ax[2].set_ylabel(\"$p(\\sigma|data)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CuPy Implementation\n",
    "\n",
    "This implementation is equivalent to the [numpy one](#basic-numpy) but using the `CuPy` [library](https://github.com/cupy/cupy)\n",
    "\n",
    "The main difference with previous approach is that in `CuPy` we can't make use of the `scipy` library and therefore we need to reinvent the wheel a bit and create the pdf of a normal distribution that is happy to work with `CuPy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "observations_cp = cp.asarray(observations_np, dtype=cp.float32)\n",
    "\n",
    "def cp_normal(mu_grid, sigma_grid, observations_grid):\n",
    "    \"\"\"\n",
    "    Compute the pdf of a normal distribution\n",
    "\n",
    "    Arguments:\n",
    "        - mu_grid (cp.ndarray): the space of parameters for the mean as a 101x101x50 array\n",
    "        - sigma_grid (cp.ndarray): the space of parameters for the std as a 101x101x50 array\n",
    "        - observations_grid (cp.ndarray): the space of observations as a 101x101x50 array\n",
    "\n",
    "    Returns:\n",
    "        - a 101x101x101 cp.ndarray with the densities per triplet mu, sigma & x\n",
    "    \"\"\"\n",
    "    normalization = cp.sqrt(2 * cp.pi) * sigma_grid\n",
    "    return cp.exp(-0.5 * ((observations_grid - mu_grid) / sigma_grid) ** 2) / normalization\n",
    "\n",
    "\n",
    "def cupy_estimator(observations):\n",
    "    \"\"\"\n",
    "    Compute the posterior distribution of a set of observations.\n",
    "\n",
    "    Arguments:\n",
    "        - observations (cp.ndarray): a CuPy array containing the the observations\n",
    "\n",
    "    Returns:\n",
    "        - a tuple of cp.ndarray:\n",
    "            - posterior (cp.ndarray[101]): the posterior probability of the observations.\n",
    "            - mu_range (cp.ndarray[101]): the range of parameters used for the mean.\n",
    "            - sigma_range (cp.ndarray[101]): the range of parameters used for the std.\n",
    "    \"\"\"\n",
    "    # Define ranges\n",
    "    grid_size = 101\n",
    "    mu_range = cp.linspace(18, 22, grid_size, dtype=cp.float32)\n",
    "    sigma_range = cp.linspace(1, 3, grid_size, dtype=cp.float32)\n",
    "\n",
    "    # Build the outer product of mu, sigma and observations.\n",
    "    mu_grid, sigma_grid, observations_grid = cp.meshgrid(mu_range, sigma_range, observations)\n",
    "\n",
    "    # Build the prior\n",
    "    prior = cp.ones((grid_size, grid_size))\n",
    "\n",
    "    # compute the likelihood. In this case we can't use the scipy norm as it only works with\n",
    "    # numpy arrays, so we will need to craft our own.\n",
    "    densities = cp_normal(mu_grid, sigma_grid, observations_grid)\n",
    "    likes = densities.prod(axis=2)\n",
    "\n",
    "    # Compute posterior\n",
    "    posterior = likes * prior\n",
    "    posterior /= posterior.sum()\n",
    "\n",
    "    # Return the expectations\n",
    "    return posterior, mu_range, sigma_range\n",
    "\n",
    "posterior, mu_range, sigma_range = cupy_estimator(observations_cp)\n",
    "print(\n",
    "    cp.sum(mu_range * posterior.sum(axis=1)),\n",
    "    cp.sum(sigma_range * posterior.sum(axis=0))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "As a final step we run some benchmarks to realise the improvement. Note that the numbers got here may differ from what you actually get in your setup. We can check that:\n",
    "- Numpy: 31.2ms\n",
    "- CuPY: .969ms (CPU) + 1.4ms (GPU) = 2.36ms\n",
    "\n",
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ms ± 4.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_estimator(observations_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy_estimator      :    CPU:   767.068 us   +/- 84.480 (min:   630.017 / max:   909.773) us     GPU-0:  1241.259 us   +/- 232.582 (min:   985.696 / max:  1724.416) us"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cupyx.profiler import benchmark\n",
    "benchmark(cupy_estimator, (observations_cp, ), n_repeat=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-grid-estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
